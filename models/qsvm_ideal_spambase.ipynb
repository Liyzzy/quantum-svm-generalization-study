{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5070b",
   "metadata": {},
   "source": [
    "#### Quantum SVM Ideal Condition Impelementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58776d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Qiskit Imports\n",
    "from qiskit.circuit.library import zz_feature_map\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "# from qiskit.primitives import StatevectorSampler as Sampler\n",
    "# from qiskit_machine_learning.state_fidelities import ComputeUncompute\n",
    "# from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "# Since this model is in the ideal condition - we use the statevector\n",
    "from qiskit.circuit import Parameter, ParameterVector\n",
    "from qiskit.circuit.library import unitary_overlap\n",
    "\n",
    "# Import StatevectorSampler as our sampler\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit.primitives import StatevectorSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa1dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "Aer: 0.17.2\n"
     ]
    }
   ],
   "source": [
    "# Checking Qiskit Version ONLY !!!\n",
    "import qiskit\n",
    "qiskit.version.get_version_info()\n",
    "import qiskit_aer\n",
    "print(qiskit.__version__)\n",
    "print(\"Aer:\", qiskit_aer.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c895eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Spambase Column Names ---\n",
    "spambase_columns = [\n",
    "    \"word_freq_make\",\n",
    "    \"word_freq_address\",\n",
    "    \"word_freq_all\",\n",
    "    \"word_freq_3d\",\n",
    "    \"word_freq_our\",\n",
    "    \"word_freq_over\",\n",
    "    \"word_freq_remove\",\n",
    "    \"word_freq_internet\",\n",
    "    \"word_freq_order\",\n",
    "    \"word_freq_mail\",\n",
    "    \"word_freq_receive\",\n",
    "    \"word_freq_will\",\n",
    "    \"word_freq_people\",\n",
    "    \"word_freq_report\",\n",
    "    \"word_freq_addresses\",\n",
    "    \"word_freq_free\",\n",
    "    \"word_freq_business\",\n",
    "    \"word_freq_email\",\n",
    "    \"word_freq_you\",\n",
    "    \"word_freq_credit\",\n",
    "    \"word_freq_your\",\n",
    "    \"word_freq_font\",\n",
    "    \"word_freq_000\",\n",
    "    \"word_freq_money\",\n",
    "    \"word_freq_hp\",\n",
    "    \"word_freq_hpl\",\n",
    "    \"word_freq_george\",\n",
    "    \"word_freq_650\",\n",
    "    \"word_freq_lab\",\n",
    "    \"word_freq_labs\",\n",
    "    \"word_freq_telnet\",\n",
    "    \"word_freq_857\",\n",
    "    \"word_freq_data\",\n",
    "    \"word_freq_415\",\n",
    "    \"word_freq_85\",\n",
    "    \"word_freq_technology\",\n",
    "    \"word_freq_1999\",\n",
    "    \"word_freq_parts\",\n",
    "    \"word_freq_pm\",\n",
    "    \"word_freq_direct\",\n",
    "    \"word_freq_cs\",\n",
    "    \"word_freq_meeting\",\n",
    "    \"word_freq_original\",\n",
    "    \"word_freq_project\",\n",
    "    \"word_freq_re\",\n",
    "    \"word_freq_edu\",\n",
    "    \"word_freq_table\",\n",
    "    \"word_freq_conference\",\n",
    "    \"char_freq_;\",\n",
    "    \"char_freq_(\",\n",
    "    \"char_freq_[\",\n",
    "    \"char_freq_!\",\n",
    "    \"char_freq_$\",\n",
    "    \"char_freq_#\",\n",
    "    \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\",\n",
    "    # finally the target label column:\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "# --- 1. Load the Spambase Dataset ---\n",
    "file_path = r'C:\\Users\\User\\Documents\\MyProjects\\FYP_ResearchProject\\data\\spambase\\spambase.data'\n",
    "df = pd.read_csv(file_path, header=None, names=spambase_columns)\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beebe707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Separate features and target\n",
    "X = df.drop('label', axis=1) # Columns axis 1, Rows axis 2 - just additional info\n",
    "y = df['label']\n",
    "\n",
    "# Now got : \n",
    "# Features - X\n",
    "# Target - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# test_size - 0.3 means 30% as test set\n",
    "# random_state - ensures the random shuffling is the same every time the code runs\n",
    "# if its random, the result will be different and other people might ended up getting different results as well\n",
    "# stratify=y - nsures fairness when comparing classical SVM vs QSVM, especially if dataset is imbalanced (like more spam than non-spam emails).\n",
    "# Look at the labels in y, calculate the percentage of each class (like 70% Class A and 30% Class B), and make sure the new training set and testing set both keep that exact same 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85da0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStandardScaler - as mentioned before, it normalizes the feature to mean 0, std 1\\nPCA - reduces dimensionality to 4 principal components, 4 also because to match with the 4 qubit feature map\\nPCA - also why choose change into 4 components is to match 4 qubits of ZZFeatureMap\\nAlso (Binary Classification) is just the classification between two classes. It does nothing to amount of qubits\\n\\nData leakage - information from test sets sneaks into the training process, makes model look better because it looks like it seen some information\\n\\nAdditional Info:\\nWhy only scale and PCA the features x and not labels y\\n- X is because they are numerical\\n- So need better format of features (same range so they dont dominate), and reduce number of features to match the number of qubits\\n\\n- Y are class identifiers\\n- Not features \\n- If scaled they it destroys their meaning\\n\\nSummary :\\nScale + PCA → features (X)\\nDo not touch → labels (y)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling and PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "n_components = 4\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\"\"\"\n",
    "StandardScaler - as mentioned before, it normalizes the feature to mean 0, std 1\n",
    "PCA - reduces dimensionality to 4 principal components, 4 also because to match with the 4 qubit feature map\n",
    "PCA - also why choose change into 4 components is to match 4 qubits of ZZFeatureMap\n",
    "Also (Binary Classification) is just the classification between two classes. It does nothing to amount of qubits\n",
    "\n",
    "Data leakage - information from test sets sneaks into the training process, makes model look better because it looks like it seen some information\n",
    "\n",
    "Additional Info:\n",
    "Why only scale and PCA the features x and not labels y\n",
    "- X is because they are numerical\n",
    "- So need better format of features (same range so they dont dominate), and reduce number of features to match the number of qubits\n",
    "\n",
    "- Y are class identifiers\n",
    "- Not features \n",
    "- If scaled they it destroys their meaning\n",
    "\n",
    "Summary :\n",
    "Scale + PCA → features (X)\n",
    "Do not touch → labels (y)\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec912a",
   "metadata": {},
   "source": [
    "#### Quantum Kernel Definition and Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, A manually constructed Quantum Kernel Computation is done.\n",
    "# Use ZZFeatureMap\n",
    "# Define quantum feature map\n",
    "feature_map = zz_feature_map(feature_dimension=X_train_pca.shape[1], reps=2, entanglement='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the problem using state vector primitives\n",
    "sampler = StatevectorSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5482cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing empty kernel matrices\n",
    "# For storing multiple outputs\n",
    "\n",
    "# retrieves the number of samples from training and testing features\n",
    "num_train = X_train_pca.shape[0]\n",
    "num_test = X_test_pca.shape[0]\n",
    "\n",
    "# makes kernel matrices to make sure they have correct dimension\n",
    "matrix_train_ideal = np.zeros((num_train, num_train)) # a square matrix of size ntrain x ntrain\n",
    "matrix_test_ideal = np.zeros((num_test, num_train)) # n_test x n_train\n",
    "\n",
    "# These matrix are then be used to store the calculated overlaps ?\n",
    "\n",
    "# np.zeros - initializing with the value 0\n",
    "# np.full(()) - initializing with the value Not a Number NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f403dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating training kernel matrix (manual BATCHED method)...\n"
     ]
    }
   ],
   "source": [
    "# --- Manually Batched Kernel Computation (Faster than the original manual method) ---\n",
    "\n",
    "print(\"\\nCalculating training kernel matrix (manual BATCHED method)...\")\n",
    "start_time_kernel_train = time.time()\n",
    "\n",
    "# 1. Create a list to hold all the circuits\n",
    "circuits_to_run = []\n",
    "# Create a list to store the (i, j) indices for each circuit\n",
    "index_map = []\n",
    "\n",
    "for i in range(num_train):\n",
    "    for j in range(i + 1, num_train): # Only upper triangle\n",
    "        # Create circuits for the two data points\n",
    "        circuit_i = feature_map.assign_parameters(X_train_pca[i])\n",
    "        circuit_j = feature_map.assign_parameters(X_train_pca[j])\n",
    "\n",
    "        # Create the overlap circuit and add it to our batch list\n",
    "        overlap_circuit = unitary_overlap(circuit_i, circuit_j)\n",
    "        overlap_circuit.measure_all()\n",
    "        circuits_to_run.append(overlap_circuit)\n",
    "        index_map.append((i, j))\n",
    "\n",
    "# 2. Run the ENTIRE batch of circuits in a single call\n",
    "print(f\"Generated {len(circuits_to_run)} circuits. Running the batch job...\")\n",
    "batched_results = sampler.run(circuits_to_run).result()\n",
    "\n",
    "# 3. Process the results and populate the matrix\n",
    "for k, result in enumerate(batched_results):\n",
    "    # Get the (i, j) indices for this result\n",
    "    i, j = index_map[k]\n",
    "    \n",
    "    probabilities = result.data.meas.get_probabilities()\n",
    "    kernel_value = probabilities.get(0, 0.0)\n",
    "    \n",
    "    matrix_train_ideal[i, j] = kernel_value\n",
    "    matrix_train_ideal[j, i] = kernel_value # Symmetric\n",
    "\n",
    "# 4. Fill the diagonal\n",
    "np.fill_diagonal(matrix_train_ideal, 1.0)\n",
    "\n",
    "end_time_kernel_train = time.time()\n",
    "print(f\"Training kernel matrix calculated in {end_time_kernel_train - start_time_kernel_train:.2f} seconds.\")\n",
    "\n",
    "# You would then repeat this entire batching process for the test matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute the Test Kernel Matrix ---\n",
    "# This loop computes the similarity between every test point and every training point.\n",
    "print(\"Calculating test kernel matrix...\")\n",
    "start_time_kernel_test = time.time()\n",
    "\n",
    "for i in range(num_test):\n",
    "    for j in range(num_train):\n",
    "        circuit_i = feature_map.assign_parameters(X_test_pca[i])\n",
    "        circuit_j = feature_map.assign_parameters(X_train_pca[j])\n",
    "        \n",
    "        overlap_circuit = unitary_overlap(circuit_i, circuit_j)\n",
    "        overlap_circuit.measure_all()\n",
    "        \n",
    "        job = sampler.run(overlap_circuit, shots=1024)\n",
    "        result = job.result()\n",
    "        counts = result.quasi_dists[0].get_probabilities()\n",
    "        \n",
    "        kernel_value = counts.get(0, 0.0)\n",
    "        matrix_test_ideal[i, j] = kernel_value\n",
    "\n",
    "end_time_kernel_test = time.time()\n",
    "print(f\"Test kernel matrix calculated in {end_time_kernel_test - start_time_kernel_test:.2f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training QSVM with Manually Computed Ideal Kernel ---\")\n",
    "# This part is identical to before, as scikit-learn only needs the final matrix.\n",
    "start_time_qsvm_train = time.time()\n",
    "\n",
    "qsvm_ideal = SVC(kernel='precomputed')\n",
    "param_grid_qsvm = {'C': [0.1, 1, 10, 100]}\n",
    "grid_qsvm_ideal = GridSearchCV(qsvm_ideal, param_grid_qsvm, cv=5, verbose=0)\n",
    "grid_qsvm_ideal.fit(matrix_train_ideal, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c601adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_qsvm_ideal = grid_qsvm_ideal.best_estimator_\n",
    "end_time_qsvm_train = time.time()\n",
    "print(f\"Best parameters for Ideal QSVM: {grid_qsvm_ideal.best_params_}\")\n",
    "print(f\"Training time for Ideal QSVM: {end_time_qsvm_train - start_time_qsvm_train:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation for Ideal QSVM ---\n",
    "y_train_pred_qsvm = best_qsvm_ideal.predict(matrix_train_ideal)\n",
    "y_test_pred_qsvm = best_qsvm_ideal.predict(matrix_test_ideal)\n",
    "\n",
    "train_accuracy_qsvm = accuracy_score(y_train, y_train_pred_qsvm)\n",
    "test_accuracy_qsvm = accuracy_score(y_test, y_test_pred_qsvm)\n",
    "gen_gap_qsvm = abs(train_accuracy_qsvm - test_accuracy_qsvm)\n",
    "\n",
    "print(\"--- Ideal QSVM Evaluation (Spambase) ---\")\n",
    "print(f\"Training Accuracy: {train_accuracy_qsvm:.4f}\")\n",
    "print(f\"Test Accuracy:     {test_accuracy_qsvm:.4f}\")\n",
    "print(f\"Generalization Gap: {gen_gap_qsvm:.4f}\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred_qsvm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypproj (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
